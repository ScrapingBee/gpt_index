{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "30146ad2-f165-4f4b-ae07-fe6597a2964f",
   "metadata": {},
   "source": [
    "# ScrapingBee Demo\n",
    "\n",
    "Demonstrates our web page reader through ScrapingBee web scraping API. Useful for JS heavy websites."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2315a154-f72d-4447-b1eb-cde9b66868cb",
   "metadata": {},
   "source": [
    "#### Using ScrapingBeeReader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "87bf7ecd-50cd-47da-9f0e-bc48d7ae45d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from gpt_index import GPTListIndex, ScrapingBeeReader\n",
    "from IPython.display import Markdown, display\n",
    "import os\n",
    "\n",
    "SCRAPINGBEE_API_KEY = \"YOUR-SCRAPINGBEE-API-KEY\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b6de3929-51eb-4064-b4b6-c203bb6debc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# NOTE: the html_to_text=True option requires html2text to be installed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "663403de-2e6e-4340-ab8f-8ee681bc06aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "documents = ScrapingBeeReader(api_key=SCRAPINGBEE_API_KEY, html_to_text=True, render_js=True).load_data([\"https://www.scrapingbee.com\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b8cd183a-2423-4a3e-ad92-dfe89ed5454e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Document(text='[ ![ScrapingBee logo](/images/logo.svg) ](/)\\n\\n__\\n\\n  * [Login](https://app.scrapingbee.com/account/login)\\n  * [Sign Up](https://app.scrapingbee.com/account/register)\\n\\n  * [Pricing](/#pricing)\\n  * [FAQ](/#faq)\\n  * [Blog](/blog/)\\n  * Other Features\\n    * [Screenshots](/features/screenshot/)\\n    * [Google search API](/features/google/)\\n    * [Data extraction](/features/data-extraction/)\\n    * [JavaScript scenario](/features/javascript-scenario/)\\n    * [No code web scraping](/features/make/)\\n  * Developers\\n    * [Tutorials](/tutorials)\\n    * [Documentation](/documentation/)\\n    * [Knowledge Base](https://help.scrapingbee.com/en/)\\n\\n# Tired of getting blocked while scraping the web?\\n\\nThe ScrapingBee web scraping API handles headless browsers and rotates proxies\\nfor you.\\n\\n[Try ScrapingBee for Free](https://app.scrapingbee.com/account/register)\\n\\n[\\n![](https://assets.capterra.com/badge/8e8d5c3accb58b7f7730306fa1940e8b.svg?v=2136249&p=195060)\\n](https://www.capterra.com/reviews/195060/ScrapingBee?utm_source=vendor&utm_medium=badge&utm_campaign=capterra_reviews_badge)\\nbased on 40+ reviews.\\n\\n![](/images/landing/hero_illustration.svg)\\n\\n![](/images/landing/feature_headless.svg)\\n\\n## Render your web page as if it were a real browser.\\n\\nWe manage thousands of headless instances using the latest Chrome version.\\nFocus on extracting the data you need, not dealing with inefficient headless\\nbrowsers.\\n\\n__Latest Chrome version __Fast, no matter what!\\n\\n> \"ScrapingBee simplified our **day-to-day marketing and engineering\\n> operations a lot** . We no longer have to worry about managing our own fleet\\n> of headless browsers, and we no longer have to spend days sourcing the right\\n> proxy provider\" ![Mike Ritchie](/images/testimonials/mike.png) **Mike\\n> Ritchie** CEO @ [SeekWell](https://seekwell.io)\\n\\n![](/images/landing/feature_rendering.svg)\\n\\n## Render JavaScript to scrape any website.\\n\\nWith JavaScript rendering, a simple parameter enables you to scrape any web\\npage, even single-page applications using React, AngularJS, Vue.js, or any\\nother libraries.\\n\\n__Custom JavaScript snippet __All JavaScript libraries supported\\n\\n> \"ScrapingBee is helping us scrape **many job boards and company websites**\\n> without having to deal with proxies or chrome browsers. It drastically\\n> simplified our data pipeline\" ![Russel\\n> Taylor](/images/testimonials/russel.jpeg) **Russel Taylor** CEO @\\n> [HelloOutbound](https://hellooutbound.com)\\n\\n![](/images/landing/feature_proxies.svg)\\n\\n## Rotate proxies to  \\nbypass rate limiting.\\n\\nThanks to our large proxy pool, you can bypass rate limiting while scraping\\nweb pages, hiding your bots and reducing the chances of being blocked.\\n\\n__Large proxy pool __IP geolocation __Automatic proxy rotation\\n\\n> \"ScrapingBee **clear documentation, easy-to-use API, and great success\\n> rate** made it a no-brainer.\" ![Dominic\\n> Phillips](/images/testimonials/dominic.jpeg) **Dominic Phillips** Co-Founder\\n> @ [CodeSubmit](https://codesubmit.io)\\n\\n### Six ways to use ScrapingBee for web harvesting\\n\\nWondering how our customers use our web scraping API?\\n\\nFrom a general web scrape to JavaScript rendering, our simple API does it all.\\n\\n![](/images/icons/icon01.svg)\\n\\n### 1\\\\. General Web Scraping\\n\\nScrapingBee web scraping API works great for general web scraping tasks like\\nreal estate scraping, price-monitoring, extracting reviews without getting\\nblocked. [documentation](/documentation/)\\n\\n![](/images/icons/data.svg)\\n\\n### 2\\\\. Data Extraction\\n\\nGetting HTML is cool, getting formatted JSON data is better. Thanks to our\\neasy-to-use extraction rules, get just the data you need with one simple API\\ncall. [learn more](/features/data-extraction)\\n\\n![](/images/icons/js.svg)\\n\\n### 3\\\\. JavaScript scenario\\n\\nIf you need to click, scroll, wait for some elements to appear or just run\\nsome custom JavaScript code on the website you want to scrape, check our JS\\nscenario feature. [learn more](/features/javascript-scenario)\\n\\n![](/images/icons/picture-1.svg)\\n\\n### 4\\\\. Screenshots\\n\\nNeed a screenshot of that website and not HTML? You can do this very easily\\nwith our screenshot feature. We also support full page and partial\\nscreenshots! [learn more](/features/screenshot)\\n\\n![](/images/icons/search.svg)\\n\\n### 5\\\\. Search Engine Result Page\\n\\nScraping search engine result pages is extremely painful because of rate\\nlimits. Thanks to our Google search API, it\\'s now easier than ever. [learn\\nmore](/features/google)\\n\\n![](/images/icons/nocode.svg)\\n\\n### 6\\\\. No code web scraping\\n\\nIf coding is not your thing, you can leverage our Make integration to create\\ncustom web scraping engines without writing a single line of code! [learn\\nmore](/features/make)\\n\\n### You\\'re in great company.\\n\\n500+ customers all around the globe use ScrapingBee to solve their web\\nscraping needs.\\n\\n> \"Excellent service, glad we made the switch! We could always dedicate\\n> resources and build our own systems for everything... or we could simply\\n> call the scrapingBee API and focus on the data. It\\'s makes our work so much\\n> easier.\" **Daniel L ★★★★★  ** Lead dev [(see it on\\n> Capterra)](https://www.capterra.com/p/195060/ScrapingBee/reviews/2933766/)\\n\\n> \"Scrapingbee helps us to retrieve information from sites that use very\\n> sophisticated mechanism to block unwanted traffic, we were struggling with\\n> those sites for some time now and **I\\'m very glad that we found\\n> ScrapingBee.**\" ![](/images/testimonials/anton.jpeg) **Anton R ★★★★★  ** CTO\\n> [(see it on\\n> Capterra)](https://www.capterra.com/p/195060/ScrapingBee/reviews/3145309/)\\n\\n> \"So easy to set-up, straightforward and performance. They are reachable and\\n> kind, they introduced us properly their tool and offered the **best solution\\n> for our need**.\" ![](/images/testimonials/maxime_2.jpeg) **Maxime Y ★★★★★\\n> ** Product Manager @ NordFolk [(see it on\\n> Capterra)](https://www.capterra.com/p/195060/ScrapingBee/reviews/2945567/)\\n\\n> \"I\\'m a PhD candidate with absolutely no web scraping experience and needed\\n> to scrape some data for a dissertation project. **ScrapingBee helped me get\\n> the job done quickly and easily**. Excellent customer support too. Couldn\\'t\\n> be happier!\" **Sam ★★★★★  ** PhD candidate [(see it on\\n> Capterra)](https://www.capterra.com/p/195060/ScrapingBee/reviews/2994189/)\\n\\n> \"Great SaaS tool for legitimate scraping and data extraction. **ScrapingBee\\n> makes it easy to automatically pull down data from the sites** that publish\\n> periodic data in a human-readable format.\"\\n> ![](/images/testimonials/andy.jpeg) **Andy Hawkes** Founder\\n> @[Loadster](https://loadster.app)\\n\\n> \"I regularly use ScrapingBee to **gather the data I need** for my blog posts\\n> about Chinese aerospace. It allows me not to worry about sourcing the right\\n> proxy provider and to focus on writing valuable content\"\\n> ![](/images/testimonials/jean.jpg) **Jean Deville** Founder @[DongFang\\n> Hour](https://dongfanghour.com)\\n\\n> \"Fantastic service: works flawlessly, best support I\\'ve experienced. It just\\n> works: and **its parsing meta-language is wonderfully powerful**. Most\\n> importantly, the support I\\'ve received has been superlative.\" **Mike P.\\n> ★★★★★  ** VP [(see it on\\n> Capterra)](https://www.capterra.com/p/195060/ScrapingBee/reviews/3091956/)\\n\\n> \"Good experience. I found this proxy service more effective compared to\\n> previous ones that were being used. It is fast and efficient.\" **Aayushi\\n> ★★★★★  ** Senior analyst [(see it on\\n> Capterra)](https://www.capterra.com/p/195060/ScrapingBee/reviews/3227885/)\\n\\n> \"Excellent service, glad we made the switch! We could always dedicate\\n> resources and build our own systems for everything... or we could simply\\n> call the scrapingBee API and focus on the data. It\\'s makes our work so much\\n> easier.\" **Daniel L ★★★★★  ** Lead dev [(see it on\\n> Capterra)](https://www.capterra.com/p/195060/ScrapingBee/reviews/2933766/)\\n\\n> \"Scrapingbee helps us to retrieve information from sites that use very\\n> sophisticated mechanism to block unwanted traffic, we were struggling with\\n> those sites for some time now and **I\\'m very glad that we found\\n> ScrapingBee.**\" ![](/images/testimonials/anton.jpeg) **Anton R ★★★★★  ** CTO\\n> [(see it on\\n> Capterra)](https://www.capterra.com/p/195060/ScrapingBee/reviews/3145309/)\\n\\n> \"So easy to set-up, straightforward and performance. They are reachable and\\n> kind, they introduced us properly their tool and offered the **best solution\\n> for our need**.\" ![](/images/testimonials/maxime_2.jpeg) **Maxime Y ★★★★★\\n> ** Product Manager @ NordFolk [(see it on\\n> Capterra)](https://www.capterra.com/p/195060/ScrapingBee/reviews/2945567/)\\n\\n> \"I\\'m a PhD candidate with absolutely no web scraping experience and needed\\n> to scrape some data for a dissertation project. **ScrapingBee helped me get\\n> the job done quickly and easily**. Excellent customer support too. Couldn\\'t\\n> be happier!\" **Sam ★★★★★  ** PhD candidate [(see it on\\n> Capterra)](https://www.capterra.com/p/195060/ScrapingBee/reviews/2994189/)\\n\\n> \"Great SaaS tool for legitimate scraping and data extraction. **ScrapingBee\\n> makes it easy to automatically pull down data from the sites** that publish\\n> periodic data in a human-readable format.\"\\n> ![](/images/testimonials/andy.jpeg) **Andy Hawkes** Founder\\n> @[Loadster](https://loadster.app)\\n\\n> \"I regularly use ScrapingBee to **gather the data I need** for my blog posts\\n> about Chinese aerospace. It allows me not to worry about sourcing the right\\n> proxy provider and to focus on writing valuable content\"\\n> ![](/images/testimonials/jean.jpg) **Jean Deville** Founder @[DongFang\\n> Hour](https://dongfanghour.com)\\n\\n> \"Fantastic service: works flawlessly, best support I\\'ve experienced. It just\\n> works: and **its parsing meta-language is wonderfully powerful**. Most\\n> importantly, the support I\\'ve received has been superlative.\" **Mike P.\\n> ★★★★★  ** VP [(see it on\\n> Capterra)](https://www.capterra.com/p/195060/ScrapingBee/reviews/3091956/)\\n\\n> \"Good experience. I found this proxy service more effective compared to\\n> previous ones that were being used. It is fast and efficient.\" **Aayushi\\n> ★★★★★  ** Senior analyst [(see it on\\n> Capterra)](https://www.capterra.com/p/195060/ScrapingBee/reviews/3227885/)\\n\\n> \"Excellent service, glad we made the switch! We could always dedicate\\n> resources and build our own systems for everything... or we could simply\\n> call the scrapingBee API and focus on the data. It\\'s makes our work so much\\n> easier.\" **Daniel L ★★★★★  ** Lead dev [(see it on\\n> Capterra)](https://www.capterra.com/p/195060/ScrapingBee/reviews/2933766/)\\n\\n____\\n\\n## Simple, transparent pricing.\\n\\nCancel anytime, no questions asked!\\n\\nAPI Credits ![](/images/icons/icon-info-grey.svg)\\n\\nConcurrent requests ![](/images/icons/icon-info-grey.svg)\\n\\nJavaScript rendering ![](/images/icons/icon-info-grey.svg)\\n\\nRotating & Premium Proxies ![](/images/icons/icon-info-grey.svg)\\n\\nGeotargeting ![](/images/icons/icon-info-grey.svg)\\n\\nScreenshots, Extraction Rules, Google Search API\\n\\nPriority Email Support\\n\\nDedicated Account Manager\\n\\nTeam Management ![](/images/icons/icon-info-grey.svg)\\n\\nRecommended **Freelance** $49/mo\\n\\n150,000\\n\\n5\\n\\n__\\n\\n__\\n\\n__\\n\\n__\\n\\n-\\n\\n-\\n\\n-\\n\\n[Try now](https://app.scrapingbee.com/account/register)\\n\\nRecommended **Startup** $99/mo\\n\\n1,000,000\\n\\n50\\n\\n__\\n\\n__\\n\\n__\\n\\n__\\n\\n__\\n\\n-\\n\\n-\\n\\n[Try now](https://app.scrapingbee.com/account/register)\\n\\nRecommended **Business** $249/mo\\n\\n3,000,000\\n\\n100\\n\\n__\\n\\n__\\n\\n__\\n\\n__\\n\\n__\\n\\n__\\n\\n__\\n\\n[Try now](https://app.scrapingbee.com/account/register)\\n\\nRecommended **Business +** $599+/mo\\n\\n9,000,000+\\n\\n200+\\n\\n__\\n\\n__\\n\\n__\\n\\n__\\n\\n__\\n\\n__\\n\\n__\\n\\n[Try now](https://app.scrapingbee.com/account/register)\\n\\nNeed more credits and concurrency per month?\\n\\nGet in touch!\\n\\nNot sure what plan you need? [Try ScrapingBee with 1000 free API\\ncalls.](https://app.scrapingbee.com/account/register)\\n\\n(No credit card required)\\n\\n## Developers are asking...\\n\\n**What happens if a request fails?** We only charge for successful requests,\\ni.e returning with a 200 or 404 status code.\\n\\n**I need more than 2,500,000 credits per month !** We got you covered! Just\\ncontact us at [contact@scrapingbee.com](mailto:contact@scrapingbee.com), we\\nwill chat and create a custom plan for you!\\n\\n**I need to scroll or click on a button on the page I want to scrape** No\\nproblem, you can pass any JavaScript snippet that needs to be executed by\\nusing our js_scenario parameter. [Learn more about JavaScript\\nsnippets](/documentation/#javascript-execution).\\n\\n**I\\'m not a developer, can you create custom scraping scripts for me?** We\\ndon\\'t create _custom scraping_ scripts, however we will gladly write some code\\nsnippets helping you to use our most powerful features: [data\\nextraction](/documentation/data-extraction/) and [javascript\\nscenario](/documentation/js-scenario/).\\n\\n**What is an API Credit?** Each plan gives a certain amount of API credits per\\nmonth. Depending on the parameters you use with your API calls it will cost\\nyou from one to several credits. By default, each request costs 5 credits\\nbecause JavaScript rendering is enabled by default. [Learn more about requests\\ncosts](/documentation/#Pricing).\\n\\n**What happens if I run out of credits before the end of my current\\nsubscription?** No worries, we got you covered. If you temporarily need more\\ncredits, you can do two things. Either upgrade your plan for the current\\nmonth. You will be able to downgrade it later whenever you want. Or you can\\nearly renew your current subscription in two clicks.\\n\\n**Can I cancel my plan any time?** Yes, you can cancel your subscription plan\\nat any time. It can be done in less than 30 seconds from your dashboard.\\n\\n## Who are we?\\n\\nDevelopers, Developers, Developers!\\n\\nYou can read the full-story [here](/journey-to-one-million-arr/).\\n\\n![Kevin Sahin](/images/team/kevin.png)\\n\\n**Kevin Sahin** Co-Founder\\n\\nKevin is a web scraping expert and author of The Java Web Scraping Handbook.\\nHe\\'s been involved in many web scraping projects, for banks, startups, and\\nE-commerce stores. He now handles all the marketing at ScrapingBee.\\n\\n![Pierre de Wulf](/images/team/pierre.png)\\n\\n**Pierre de Wulf** Co-Founder\\n\\nPierre is a data-engineer. He\\'s been involved in many startups, in the US and\\nin Europe. Previously, with Kevin, he co-founded PricingBot a price-monitoring\\nservice for E-commerce. He now takes care of the tech / product side of\\nScrapingBee.\\n\\n![Etienne Ellie](/images/team/etienne.png)\\n\\n**Etienne Ellie** Lead Developer\\n\\nEtienne is a senior developer with a wide range of experiences. From\\ndeveloping a product from the ground-up at a fast-scaling startup to computer\\nvision for the aerospace industry, he\\'s now in charge of everything technical\\nat ScrapingBee. He also gives some help with the trickiest support tickets.\\n\\n## Contact us\\n\\nGot any questions? Don\\'t hesitate to reach\\n\\n[Get in touch](https://forms.reform.app/Id4eCS/work-with-us/6wyt1u)\\n\\n## Ready to get started?\\n\\nGet access to 1,000 free API credits, no credit card required!\\n\\n[Try ScrapingBee for Free](https://app.scrapingbee.com/account/register)\\n\\n[![ScrapingBee](/images/logo-white.svg)](/)\\n\\nScrapingBee API handles headless browsers and rotates proxies for you.\\n\\n  * [__](https://twitter.com/ScrapingBee)\\n  * [__](https://www.linkedin.com/company/scrapingbee)\\n\\n#### Company\\n\\n  * [Team](/#about-us)\\n  * [Company\\'s journey](/journey-to-one-million-arr/)\\n  * [Blog](/blog/)\\n  * [Rebranding](/rebranding/)\\n  * [Affiliate Program](/affiliates/)\\n\\n#### Tools\\n\\n  * [Curl converter](/curl-converter/)\\n\\n#### Legal\\n\\n  * [Terms of Service](/terms-and-conditions/)\\n  * [Privacy Policy](/privacy-policy/)\\n  * [GDPR Compliance](/gdpr/)\\n  * [Data Processing Agreement](/data-processing-agreement/)\\n\\n#### Product\\n\\n  * [Features](/#features)\\n  * [Pricing](/#pricing)\\n  * [Status](https://status.scrapingbee.com)\\n\\n#### How we compare\\n\\n  * [Alternative to Crawlera](/crawlera-alternative/)\\n  * [Alternative to Luminati](/luminati-alternative/)\\n  * [Alternative to Smartproxy](/smartproxy-alternative/)\\n  * [Alternative to Oxylabs](/oxylabs-alternative/)\\n  * [Alternative to NetNut](/netnut-alternative/)\\n  * [Alternative to ScraperAPI](/scraperapi-alternative/)\\n  * [Alternatives to ScrapingBee](/scrapingbee-alternative/)\\n\\n#### No code web scraping\\n\\n  * [No code web scraping](/blog/no-code-web-scraping/)\\n  * [No code competitor monitoring](/blog/no-code-competitor-monitoring/)\\n  * [How to put scraped website data into Google Sheets](/blog/scrape-content-google-sheet/)\\n  * [Send stock prices update to Slack](/blog/no-code-stock-price-slack/)\\n  * [Scrape Amazon products\\' price with no code](/blog/nocode-amazon/)\\n  * [Scrape Amazon products\\' price with no code](/blog/nocode-amazon/)\\n  * [Extract job listings, details and salaries](/blog/no-code-job-data-extraction/)\\n\\n#### Learning Web Scraping\\n\\n  * [Web scraping questions](/webscraping-questions/)\\n  * [A guide to Web Scraping without getting blocked](/blog/web-scraping-without-getting-blocked/)\\n  * [Web Scraping Tools](/blog/web-scraping-tools/)\\n  * [Best Free Proxies](/blog/best-free-proxy-list-web-scraping/)\\n  * [Best Mobile proxies](/blog/best-mobile-4g-proxy-provider-webscraping/)\\n  * [Web Scraping vs Web Crawling](/blog/scraping-vs-crawling/)\\n  * [Rotating and residential proxies](/blog/rotating-proxies/)\\n  * [Web Scraping with Python](/blog/web-scraping-101-with-python/)\\n  * [Web Scraping with PHP](/blog/web-scraping-php/)\\n  * [Web Scraping with Java](/blog/introduction-to-web-scraping-with-java/)\\n  * [Web Scraping with Ruby](/blog/web-scraping-ruby/)\\n  * [Web Scraping with NodeJS](/blog/web-scraping-javascript/)\\n  * [Web Scraping with R](/blog/web-scraping-r/)\\n  * [Web Scraping with C#](/blog/web-scraping-csharp/)\\n  * [Web Scraping with C++](/blog/web-scraping-c++/)\\n  * [Web Scraping with Elixir](/blog/web-scraping-elixir/)\\n  * [Web Scraping with Perl](/blog/web-scraping-perl/)\\n  * [Web Scraping with Rust](/blog/web-scraping-rust/)\\n  * [Web Scraping with Go](/blog/web-scraping-go/)\\n\\n__\\n\\nCopyright © 2023\\n\\nMade in France\\n\\n', doc_id=None, embedding=None, extra_info=None)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "documents[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "26854cc3-af61-4910-ab6b-3bed6acfb447",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValidationError",
     "evalue": "1 validation error for OpenAI\n__root__\n  Did not find openai_api_key, please add an environment variable `OPENAI_API_KEY` which contains it, or pass  `openai_api_key` as a named parameter. (type=value_error)",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mValidationError\u001B[0m                           Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[10], line 1\u001B[0m\n\u001B[0;32m----> 1\u001B[0m index \u001B[38;5;241m=\u001B[39m \u001B[43mGPTListIndex\u001B[49m\u001B[43m(\u001B[49m\u001B[43mdocuments\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/Documents/Code.nosync/gpt_index/gpt_index/indices/list/base.py:54\u001B[0m, in \u001B[0;36mGPTListIndex.__init__\u001B[0;34m(self, documents, index_struct, text_qa_template, llm_predictor, **kwargs)\u001B[0m\n\u001B[1;32m     52\u001B[0m \u001B[38;5;124;03m\"\"\"Initialize params.\"\"\"\u001B[39;00m\n\u001B[1;32m     53\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mtext_qa_template \u001B[38;5;241m=\u001B[39m text_qa_template \u001B[38;5;129;01mor\u001B[39;00m DEFAULT_TEXT_QA_PROMPT\n\u001B[0;32m---> 54\u001B[0m \u001B[38;5;28;43msuper\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[38;5;21;43m__init__\u001B[39;49m\u001B[43m(\u001B[49m\n\u001B[1;32m     55\u001B[0m \u001B[43m    \u001B[49m\u001B[43mdocuments\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mdocuments\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m     56\u001B[0m \u001B[43m    \u001B[49m\u001B[43mindex_struct\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mindex_struct\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m     57\u001B[0m \u001B[43m    \u001B[49m\u001B[43mllm_predictor\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mllm_predictor\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m     58\u001B[0m \u001B[43m    \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m     59\u001B[0m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     60\u001B[0m \u001B[38;5;66;03m# NOTE: when building the list index, text_qa_template is not partially\u001B[39;00m\n\u001B[1;32m     61\u001B[0m \u001B[38;5;66;03m# formatted because we don't know the query ahead of time.\u001B[39;00m\n\u001B[1;32m     62\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_text_splitter \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_prompt_helper\u001B[38;5;241m.\u001B[39mget_text_splitter_given_prompt(\n\u001B[1;32m     63\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mtext_qa_template, \u001B[38;5;241m1\u001B[39m\n\u001B[1;32m     64\u001B[0m )\n",
      "File \u001B[0;32m~/Documents/Code.nosync/gpt_index/gpt_index/indices/base.py:81\u001B[0m, in \u001B[0;36mBaseGPTIndex.__init__\u001B[0;34m(self, documents, index_struct, llm_predictor, embed_model, docstore, index_registry, prompt_helper, chunk_size_limit, verbose, include_extra_info)\u001B[0m\n\u001B[1;32m     78\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m index_struct \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m \u001B[38;5;129;01mand\u001B[39;00m documents \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[1;32m     79\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mValueError\u001B[39;00m(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mOnly one of documents or index_struct can be provided.\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[0;32m---> 81\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_llm_predictor \u001B[38;5;241m=\u001B[39m llm_predictor \u001B[38;5;129;01mor\u001B[39;00m \u001B[43mLLMPredictor\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     82\u001B[0m \u001B[38;5;66;03m# NOTE: the embed_model isn't used in all indices\u001B[39;00m\n\u001B[1;32m     83\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_embed_model \u001B[38;5;241m=\u001B[39m embed_model \u001B[38;5;129;01mor\u001B[39;00m OpenAIEmbedding()\n",
      "File \u001B[0;32m~/Documents/Code.nosync/gpt_index/gpt_index/langchain_helpers/chain_wrapper.py:72\u001B[0m, in \u001B[0;36mLLMPredictor.__init__\u001B[0;34m(self, llm, retry_on_throttling)\u001B[0m\n\u001B[1;32m     68\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21m__init__\u001B[39m(\n\u001B[1;32m     69\u001B[0m     \u001B[38;5;28mself\u001B[39m, llm: Optional[BaseLLM] \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m, retry_on_throttling: \u001B[38;5;28mbool\u001B[39m \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mTrue\u001B[39;00m\n\u001B[1;32m     70\u001B[0m ) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[1;32m     71\u001B[0m     \u001B[38;5;124;03m\"\"\"Initialize params.\"\"\"\u001B[39;00m\n\u001B[0;32m---> 72\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_llm \u001B[38;5;241m=\u001B[39m llm \u001B[38;5;129;01mor\u001B[39;00m \u001B[43mOpenAI\u001B[49m\u001B[43m(\u001B[49m\u001B[43mtemperature\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;241;43m0\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mmodel_name\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mtext-davinci-003\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m)\u001B[49m\n\u001B[1;32m     73\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mretry_on_throttling \u001B[38;5;241m=\u001B[39m retry_on_throttling\n\u001B[1;32m     74\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_total_tokens_used \u001B[38;5;241m=\u001B[39m \u001B[38;5;241m0\u001B[39m\n",
      "File \u001B[0;32m~/.pyenv/versions/3.8.6/lib/python3.8/site-packages/pydantic/main.py:342\u001B[0m, in \u001B[0;36mpydantic.main.BaseModel.__init__\u001B[0;34m()\u001B[0m\n",
      "\u001B[0;31mValidationError\u001B[0m: 1 validation error for OpenAI\n__root__\n  Did not find openai_api_key, please add an environment variable `OPENAI_API_KEY` which contains it, or pass  `openai_api_key` as a named parameter. (type=value_error)"
     ]
    }
   ],
   "source": [
    "index = GPTListIndex(documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cfdf87a-97cb-481f-ad51-be5bf8b5217f",
   "metadata": {},
   "outputs": [],
   "source": [
    "response = index.query(\"What is ScrapingBee about?\", verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7278d033-cae3-4ddf-96bd-75ea570ca53f",
   "metadata": {},
   "outputs": [],
   "source": [
    "display(Markdown(f\"<b>{response}</b>\"))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  },
  "vscode": {
   "interpreter": {
    "hash": "c32397a35d2e76e766f80c3872b208f0c0029e8a6a9b8e2a8fe7b1641cfa009b"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
